# Cell 0
%pip -q install "numpy<2" typing_extensions>=4.9.0
%pip -q install jax==0.4.28 jaxlib==0.4.28 ml-dtypes==0.2.0 opt_einsum==3.3.0
%pip -q install dm-haiku absl-py chex==0.1.91 optax==0.2.5 toolz einops tabulate jmp networkx
%pip -q install "torch==2.2.2"
#%pip -q install "transformer_lens>=2.4.0"
%pip -q install pytorch_lightning

import sys, torch, jax, numpy
print("Python:", sys.version)
print("Torch:", torch.__version__, "CUDA:", torch.cuda.is_available())
print("JAX:", jax.__version__, "NumPy:", numpy.__version__)


# Cell 1
# Kip's change

import google.colab
IN_COLAB = True
print("Running as a Colab notebook")
%pip install transformer_lens
try:
    %pip install git+https://github.com/neelnanda-io/Tracr.git@main#egg=tracr
except:
    print("First attempt failed, trying alternative...")
    %pip install git+https://github.com/neelnanda-io/Tracr.git



# Cell 2
import os, json, numpy as np, torch
import torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader
from transformer_lens import HookedTransformer, HookedTransformerConfig
from tracr.rasp import rasp
from tracr.compiler import compiling
import pytorch_lightning as pl
import einops

SEED = 42
pl.seed_everything(SEED, workers=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# SAE config
SAE_LATENTS = 32
LR_SAE, EPOCHS_SAE = 1e-3, 5
BATCH_SAE = 64
LAMBDA_L0_LIST = [1e-2, 5e-3, 1e-3, 5e-4]
OUT_DIR = "sae_artifacts"; os.makedirs(OUT_DIR, exist_ok=True)

# ---- Transcoder (future use) ----
TX_LATENTS, TX_TAU, TX_INIT_TH = 128, 0.1, 0.5
TX_LAMBDA, TX_LR, TX_EPOCHS, TX_BS = 1e-2, 1e-3, 5, 256


# Cell 4
# Kip's change - imports for consistency
from transformer_lens import HookedTransformer, HookedTransformerConfig
import einops
import torch
import numpy as np
from tracr.rasp import rasp
from tracr.compiler import compiling


# Cell 5
# Kip's change - compile reverse RASP
def make_length():
    all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)
    return rasp.SelectorWidth(all_true_selector)

length = make_length()
opp_index = length - rasp.indices - 1
flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)
prog = rasp.Aggregate(flip, rasp.tokens)

bos = "BOS"
model = compiling.compile_rasp_to_model(
    prog,
    vocab={1, 2, 3},
    max_seq_len=5,
    compiler_bos=bos,
)

# Quick smoke test
out = model.apply([bos, 1, 2, 3])
print(f"[{prog}] Tracr output (decoded):", getattr(out, "decoded", None))


# Cell 6
# Kip's change - map Tracr → TransformerLens
n_heads = model.model_config.num_heads
n_layers = model.model_config.num_layers
d_head = model.model_config.key_size
d_mlp = model.model_config.mlp_hidden_size
act_fn = "relu"
normalization_type = "LN" if model.model_config.layer_norm else None
attention_type = "causal" if model.model_config.causal else "bidirectional"

n_ctx = model.params["pos_embed"]['embeddings'].shape[0]
d_vocab = model.params["token_embed"]['embeddings'].shape[0]
d_model = model.params["token_embed"]['embeddings'].shape[1]
d_vocab_out = d_vocab - 2  # trim BOS + PAD

cfg = HookedTransformerConfig(
    n_layers=n_layers,
    d_model=d_model,
    d_head=d_head,
    n_ctx=n_ctx,
    d_vocab=d_vocab,
    d_vocab_out=d_vocab_out,
    d_mlp=d_mlp,
    n_heads=n_heads,
    act_fn=act_fn,
    attention_dir=attention_type,
    normalization_type=normalization_type,
)
tl_model = HookedTransformer(cfg)

sd = {}
sd["pos_embed.W_pos"] = model.params["pos_embed"]['embeddings']
sd["embed.W_E"] = model.params["token_embed"]['embeddings']
sd["unembed.W_U"] = np.eye(d_model, d_vocab_out)

for l in range(n_layers):
    sd[f"blocks.{l}.attn.W_K"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/key"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.b_K"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/key"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.W_Q"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/query"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.b_Q"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/query"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.W_V"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/value"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.b_V"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/value"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.W_O"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/linear"]["w"],
        "(n_heads d_head) d_model -> n_heads d_head d_model",
        d_head=d_head, n_heads=n_heads
    )
    sd[f"blocks.{l}.attn.b_O"] = model.params[f"transformer/layer_{l}/attn/linear"]["b"]

    sd[f"blocks.{l}.mlp.W_in"] = model.params[f"transformer/layer_{l}/mlp/linear_1"]["w"]
    sd[f"blocks.{l}.mlp.b_in"] = model.params[f"transformer/layer_{l}/mlp/linear_1"]["b"]
    sd[f"blocks.{l}.mlp.W_out"] = model.params[f"transformer/layer_{l}/mlp/linear_2"]["w"]
    sd[f"blocks.{l}.mlp.b_out"] = model.params[f"transformer/layer_{l}/mlp/linear_2"]["b"]

print(sd.keys())


# Cell 7
# Kip's change - convert JAX → Torch tensors
for k, v in sd.items():
    sd[k] = torch.tensor(np.array(v))

tl_model.load_state_dict(sd, strict=False)


# Cell 8
# Kip's change - helpers
INPUT_ENCODER = model.input_encoder
OUTPUT_ENCODER = model.output_encoder

def create_model_input(input, input_encoder=INPUT_ENCODER):
    encoding = input_encoder.encode(input)
    return torch.tensor(encoding).unsqueeze(dim=0)

def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):
    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)
    decoded_output = output_encoder.decode(max_output_indices.tolist())
    decoded_output_with_bos = [bos_token] + decoded_output[1:]
    return decoded_output_with_bos


# Cell 9
# Kip's change - parity test
input = [bos, 1, 2, 3]
out = model.apply(input)
print("Original Decoding:", out.decoded)

input_tokens_tensor = create_model_input(input)
logits = tl_model(input_tokens_tensor)
decoded_output = decode_model_output(logits)
print("TransformerLens Replicated Decoding:", decoded_output)


# Cell 10
# Kip's change - layer parity checks
logits, cache = tl_model.run_with_cache(input_tokens_tensor)
for layer in range(tl_model.cfg.n_layers):
    print(f"Layer {layer} Attn Out Equality Check:",
          np.isclose(cache["attn_out", layer].detach().cpu().numpy(),
                     np.array(out.layer_outputs[2*layer])).all())
    print(f"Layer {layer} MLP Out Equality Check:",
          np.isclose(cache["mlp_out", layer].detach().cpu().numpy(),
                     np.array(out.layer_outputs[2*layer+1])).all())


# Cell 11
@torch.no_grad()
def parity_exact_match(samples=64):
    # Warm up Tracr JAX model once to avoid repeated compilation
    _ = model.apply([bos, 1, 2, 3])  
    
    ok = 0
    for _ in range(samples):
        L = np.random.randint(1, 5)
        seq = [bos] + list(np.random.choice([1, 2, 3], size=L))
        
        # TransformerLens forward
        tl_out = decode_model_output(tl_model(create_model_input(seq)))
        # Tracr forward (already compiled)
        tr_out = model.apply(seq).decoded
        
        ok += (tl_out == tr_out)
    
    print(f"TL vs Tracr exact-match: {ok}/{samples}")

parity_exact_match(64)


# Cell 12
# === Data collection: sample sequences + collect activations from ALL layers ===

@torch.no_grad()
def sample_sequences(n=8000, max_len=5):
    """Generate BOS + random content sequences, return token tensors + raw seqs."""
    X = []
    for _ in range(n):
        L = np.random.randint(1, max_len)  # 1..max_len-1 after BOS
        seq = [bos] + list(np.random.choice([1, 2, 3], size=L))
        X.append(seq)
    
    # Process sequences one by one to avoid tensor size mismatch
    token_tensors = []
    for seq in X:
        token_tensor = create_model_input(seq)[0]  # Remove batch dimension
        token_tensors.append(token_tensor)
    
    return token_tensors, X

@torch.no_grad()
def collect_activations(token_tensors, hook_point, batch=128):
    """Collect activations at a given hook_point for all sequences."""
    feats = []
    for i in range(0, len(token_tensors), batch):
        batch_tensors = token_tensors[i:i+batch]
        # Pad batch to same length
        max_len = max(t.size(0) for t in batch_tensors)
        padded_batch = []
        for t in batch_tensors:
            if t.size(0) < max_len:
                # Pad with zeros (or use attention mask later)
                pad_size = max_len - t.size(0)
                padded_t = torch.cat([t, torch.zeros(pad_size, dtype=t.dtype, device=t.device)])
            else:
                padded_t = t
            padded_batch.append(padded_t)
        
        b = torch.stack(padded_batch, dim=0)
        _, cache = tl_model.run_with_cache(b)
        act = cache[hook_point]              # (B, T, d_model)
        feats.append(act.reshape(-1, act.size(-1)))  # flatten positions
    return torch.cat(feats, dim=0)



# Cell 13
# === SAE definition + training helpers (per-layer) ===

class ActDataset(torch.utils.data.Dataset):
    def __init__(self, X): self.X = X
    def __len__(self): return self.X.size(0)
    def __getitem__(self, i): return self.X[i]

class SAE(pl.LightningModule):
    def __init__(self, d_in, d_latent=SAE_LATENTS, l1=1e-3, lr=1e-3):
        super().__init__()
        self.save_hyperparameters()
        self.enc = nn.Linear(d_in, d_latent, bias=True)
        self.dec = nn.Linear(d_latent, d_in, bias=False)
        self.l1 = l1; self.lr = lr
        nn.init.kaiming_uniform_(self.enc.weight, a=np.sqrt(5))
        nn.init.zeros_(self.dec.weight)

    def forward(self, x):
        z = F.relu(self.enc(x))
        x_hat = self.dec(z)
        return x_hat, z

    def training_step(self, batch, _):
        x = batch
        x_hat, z = self(x)
        recon = F.mse_loss(x_hat, x)
        spars = z.abs().mean()
        loss = recon + self.l1 * spars
        self.log_dict({"train/recon": recon, "train/l1": spars, "train/loss": loss}, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=self.lr)

def train_sae_for_layer(layer, l1, tag):
    """Train SAE for resid_post of a given layer."""
    hook_point = ("resid_post", layer)
    print(f"Training SAE for layer {layer} on {hook_point}")

    toks, _ = sample_sequences(n=8000, max_len=5)
    acts = collect_activations(toks, hook_point).to(device)
    print(f"Layer {layer} activations: {acts.shape}")

    # Split into train/val
    n = acts.size(0)
    idx = torch.randperm(n)
    train = acts[idx[: int(0.9*n)]]
    val   = acts[idx[int(0.9*n):]]

    train_loader = DataLoader(ActDataset(train), batch_size=BATCH_SAE, shuffle=True, num_workers=0)
    val_loader   = DataLoader(ActDataset(val),   batch_size=BATCH_SAE, shuffle=False, num_workers=0)

    model = SAE(d_in=acts.size(1), d_latent=SAE_LATENTS, l1=l1, lr=LR_SAE).to(device)
    ckpt_path = os.path.join(OUT_DIR, f"sae_layer{layer}_l1{l1}_{tag}.ckpt")

    trainer = pl.Trainer(max_epochs=EPOCHS_SAE, accelerator="auto", devices=1,
                         log_every_n_steps=10, enable_checkpointing=True, default_root_dir=OUT_DIR)
    trainer.fit(model, train_loader, val_loader)
    torch.save(model.state_dict(), ckpt_path)

    return ckpt_path, model, val


# Cell 14
# === Train & evaluate SAEs for all layers ===

print(f"\nTraining SAEs for {n_layers} layers on resid_post...")
all_artifacts = []

for layer in range(n_layers):
    layer_artifacts = []
    for i, lam in enumerate(LAMBDA_L0_LIST):
        ck, model, val = train_sae_for_layer(layer, lam, f"id{i}")
        layer_artifacts.append((lam, ck, model, val))
    all_artifacts.append(layer_artifacts)

def eval_sae(model, X, name="eval"):
    model.eval()
    with torch.no_grad():
        x_hat, z = model(X.to(device))
        recon = F.mse_loss(x_hat, X.to(device)).item()
        active_frac = (z > 1e-9).float().mean().item()
    print(f"{name}: recon={recon:.4e} | active_frac={active_frac:.3f}")
    return recon, active_frac

print("\n=== SAE Evaluation Results ===")
for layer in range(n_layers):
    print(f"\n--- Layer {layer} ---")
    for lam, ck, model, val in all_artifacts[layer]:
        eval_sae(model, val[:1024], name=f"Layer {layer}, λ={lam:g}")
